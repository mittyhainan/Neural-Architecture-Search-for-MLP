{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxLeGjNI93V-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import tqdm\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Class that defines the search space\n",
        "class MLPSearchSpace(object):\n",
        "  def __init__(self):\n",
        "    self.vocab = self.vocab_dict()\n",
        "  def vocab_dict(self):\n",
        "    #could be modified to contain cnn or more complicate structure\n",
        "    nodes = [8,16,32,64,128,256,512,1024]\n",
        "    act_funcs = ['relu','elu','tanh']\n",
        "\n",
        "    layer_params = []\n",
        "    layer_id = []\n",
        "\n",
        "    for i in range(len(nodes)):\n",
        "      for j in range(len(act_funcs)):\n",
        "        layer_params.append((nodes[i],act_funcs[j]))\n",
        "        layer_id.append(len(act_funcs)*i+j)\n",
        "    vocab = dict(zip(layer_id,layer_params))\n",
        "    return vocab\n",
        "  #randomly sample architecture from the search space. The default number of sampled \n",
        "  #architecture is 10\n",
        "  def random_sample_architecture(self,architecture_num = 10):\n",
        "    search_space_size = len(self.vocab)\n",
        "    architecture_space = []\n",
        "    for i in range(architecture_num):\n",
        "      layer_num = random.randint(3, 10)\n",
        "      sequence = [random.randint(0, search_space_size-1) for i in range(layer_num)]\n",
        "      architecture_space.append(sequence)\n",
        "    return architecture_space\n",
        "\n",
        "  #Encode architecture(list) into numerical sequence\n",
        "  def encode_sequence(self, sequence):\n",
        "    keys = list(self.vocab.keys())\n",
        "    values = list(self.vocab.values())\n",
        "    encoded_sequence = []\n",
        "    for value in sequence:\n",
        "      encoded_sequence.append(keys[values.index(value)])\n",
        "    return encoded_sequence\n",
        "  \n",
        "  #Decode numerical sequence into architecture(list)\n",
        "  def decode_sequence(self, sequence):\n",
        "    keys = list(self.vocab.keys())\n",
        "    values = list(self.vocab.values())\n",
        "    decoded_sequence = []\n",
        "    for key in sequence:\n",
        "      decoded_sequence.append(values[keys.index(key)])\n",
        "    return decoded_sequence"
      ],
      "metadata": {
        "id": "bNr-PudU9_xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class to generate model based on the given architecture\n",
        "class MLPGenerator(MLPSearchSpace):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.mlp_one_shot = True\n",
        "    self.mlp_optimizer = 'Adam'\n",
        "    self.mlp_lr = 1e-4\n",
        "    self.mlp_loss_func = 'mse'\n",
        "  \n",
        "  #create model based on the sequence. User can define the input size\n",
        "  def create_model(self,sequence,mlp_input_shape = 3):\n",
        "    layer_configs = self.decode_sequence(sequence)\n",
        "    #layer_configs = sequence\n",
        "    layers = nn.Sequential()\n",
        "    previous_layer_num = 0\n",
        "    for i,layer_conf in enumerate(layer_configs):\n",
        "      if i == 0:\n",
        "        layers.append(nn.Linear(mlp_input_shape,layer_conf[0]))\n",
        "        if layer_configs[i][1] == 'relu':\n",
        "          layers.append(nn.ReLU())\n",
        "        elif layer_configs[i][1] == 'elu':\n",
        "          layers.append(nn.ELU())\n",
        "        else:\n",
        "          layers.append(nn.Tanh())\n",
        "        previous_layer_num = layer_conf[0]\n",
        "      else:\n",
        "        layers.append(nn.Linear(previous_layer_num,layer_conf[0]))\n",
        "        previous_layer_num = layer_conf[0]\n",
        "        if layer_configs[i][1] == 'relu':\n",
        "          layers.append(nn.ReLU())\n",
        "        elif layer_configs[i][1] == 'elu':\n",
        "          layers.append(nn.ELU())\n",
        "        else:\n",
        "          layers.append(nn.Tanh())\n",
        "    layers.append(nn.Linear(previous_layer_num,2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  #Train the PINN model\n",
        "  def train_PINN(self,model,X_pinn = X_pinn, X_semigroup = X_semigroup, \n",
        "                 X_smooth = X_smooth,T = T):\n",
        "    pinn_model = PINN(X_pinn, X_semigroup, X_smooth, T, model)\n",
        "    early_loss = pinn_model.train()\n",
        "    return early_loss\n",
        "\n",
        "  #low-fidelity training for one genertaed_sequence\n",
        "  def train_model(self,model,train_dataloader):\n",
        "    #decoded_sequence = self.decode_sequence(encoded_sequence)\n",
        "    #model = self.create_model(encoded_sequence,3)\n",
        "    #one shot training -- with only one epoch\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "      # Get and prepare inputs\n",
        "      inputs, targets = data\n",
        "      inputs, targets = inputs.float(), targets.float()\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = loss_function(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "    return running_loss\n",
        "\n",
        "\n",
        "  #low-fidelity training for all generate_sequence\n",
        "  #TODO: modify to adapt to semigroup PINN\n",
        "  def low_fidelity_evaluation(self,train_dataloader,sample_space):\n",
        "    if not sample_space:\n",
        "      search_space = self.random_sample_architecture()\n",
        "    else:\n",
        "      search_space = sample_space\n",
        "    model_train_log = {}\n",
        "    model_eval_log = {}\n",
        "    architecture_history = {}\n",
        "    for i,encoded_sequence in enumerate(search_space):\n",
        "      print(\"training architecture: \", i)\n",
        "      decoded_sequence = self.decode_sequence(encoded_sequence)\n",
        "      model = self.create_model(encoded_sequence,3)\n",
        "      #low fidelity training -- with only one epoch\n",
        "      loss_function = nn.MSELoss()\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_dataloader, 0):\n",
        "        # Get and prepare inputs\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "          print(f\"batch {i + 1}] loss: {running_loss / 200:.3f}\")\n",
        "          running_loss = 0.0\n",
        "\n",
        "      print(\"training loss: \",running_loss)\n",
        "      model_train_log[f'{decoded_sequence}'] = running_loss\n",
        "      architecture_history[f'{encoded_sequence}'] = running_loss\n",
        "    return model_train_log,architecture_history"
      ],
      "metadata": {
        "id": "XKxbDi8j-DWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the controller model based a on 1-layer LSTM\n",
        "class Controller_Model(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,num_class,num_layers = 1):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTM(input_size,hidden_size,num_layers)\n",
        "    self.sequence_generator = nn.Sequential(\n",
        "        nn.Linear(hidden_size,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,num_class),\n",
        "        nn.Softmax()\n",
        "    )\n",
        "  def forward(self,input):\n",
        "    output,_ = self.lstm(input)\n",
        "    generated_sequence = self.sequence_generator(output)\n",
        "    return generated_sequence\n",
        "\n",
        "#Class to define the controller\n",
        "class Controller(MLPSearchSpace):\n",
        "  def __init__(self,max_architecture_length):\n",
        "    super().__init__()\n",
        "    self.max_length = max_architecture_length\n",
        "    self.controller_classes = len(self.vocab)\n",
        "    self.sequence_data = []\n",
        "  #create the controller model\n",
        "  def control_model(self):\n",
        "    model = Controller_Model(self.max_length,10,num_class = self.controller_classes)\n",
        "    return model\n",
        "\n",
        "  #train the controller model\n",
        "  def train_control_model(self,model,x_data,y_data,loss_func,controller_training_epoch):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    model.train()\n",
        "    #print(\"Training Controller Model\")\n",
        "    for epoch in range(controller_training_epoch):\n",
        "      print(\"Training Controller Model in epoch: \",epoch)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(x_data)\n",
        "      loss = loss_func(y_data,outputs)\n",
        "      loss.sum().backward()\n",
        "      optimizer.step()\n",
        "  #process for controller to generate architecture sequence by the given length\n",
        "  def sample_architrecture_sequences(self,model,number_of_samples,\n",
        "                                     by_prob = True):\n",
        "    samples = []\n",
        "    print(\"Generate Architecture Samples...\")\n",
        "    print('--------------------------------')\n",
        "\n",
        "    while len(samples) < number_of_samples:\n",
        "      seed = []\n",
        "      #number of layers we want in the architecture\n",
        "      while len(seed) < self.max_length:\n",
        "        seed_pad = np.pad(seed,pad_width=(0,self.max_length-len(seed)))\n",
        "        sequence = torch.Tensor(seed_pad)\n",
        "        sequence = sequence.reshape(1,len(sequence))\n",
        "        logit = model(sequence)\n",
        "        if by_prob == True:\n",
        "          #sample next architecture code by the probabilities returned by controller model\n",
        "          proba = logit.detach().numpy()[0]\n",
        "          next = np.random.choice(list(self.vocab.keys()),size=1,p=proba).item()\n",
        "        else:\n",
        "          next = torch.argmax(logit).detach().item()\n",
        "        seed.append(next)\n",
        "      #only record architecture that has not been generated before\n",
        "      if seed not in self.sequence_data:\n",
        "        samples.append(seed)\n",
        "        self.sequence_data.append(seed)\n",
        "    return samples"
      ],
      "metadata": {
        "id": "jaUU-2ya-IrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "class MLPNAS(Controller):\n",
        "  def __init__(self,max_architecture_length):\n",
        "    super().__init__(max_architecture_length)\n",
        "    self.model_generator = MLPGenerator()\n",
        "    self.controller_model = self.control_model()\n",
        "    self.controller_sampling_epochs = 3\n",
        "    self.controller_loss_alpha = 0.9\n",
        "    self.samples_per_controller_epoch = 10\n",
        "    self.controller_training_epoch = 3\n",
        "    self.data = []\n",
        "  #record the architecture evaluation history\n",
        "  def append_model_metrics(self,sequence,history):\n",
        "    self.data.append([sequence,history])\n",
        "\n",
        "  #use REINFORCE to train controller\n",
        "  def prepare_controller_data(self,sequences):\n",
        "    def to_categorical(y, num_classes):\n",
        "      return np.eye(num_classes, dtype='uint8')[y]\n",
        "    x = torch.Tensor(sequences)[:,:-1].reshape(len(sequences),1,self.max_length-1)\n",
        "    padding = torch.zeros([len(sequences),1,1])\n",
        "    x_padded = torch.concat([x,padding],dim=2)\n",
        "    y = torch.Tensor(sequences)[:,-1].type(torch.int64)\n",
        "    y = to_categorical(y,self.controller_classes)\n",
        "\n",
        "    val_loss_target = [item[1] for item in self.data[-self.samples_per_controller_epoch:]]\n",
        "    return x_padded,y,val_loss_target\n",
        "\n",
        "  #Get the discounted reward for the REINFORCE algorithm\n",
        "  def get_discounted_reward(self,rewards):\n",
        "    discounted_r = np.zeros_like(rewards,dtype=np.float32)\n",
        "    for t in range(len(rewards)):\n",
        "      running_add = 0\n",
        "      exp = 0\n",
        "      for r in rewards[t:]:\n",
        "        running_add += self.controller_loss_alpha**exp*r\n",
        "        exp += 1\n",
        "      discounted_r[t] = running_add\n",
        "    discounted_r = (discounted_r - discounted_r.mean())/discounted_r.std()\n",
        "    return discounted_r\n",
        "\n",
        "  #compute the custom loss from the expected reward \n",
        "  def custom_loss(self,target,output):\n",
        "    reward = np.array([item[1] for item in self.data[-self.samples_per_controller_epoch:]]).reshape(self.samples_per_controller_epoch, 1)\n",
        "    discounted_reward = self.get_discounted_reward(reward)\n",
        "    output = torch.Tensor(output.detach().numpy())\n",
        "    loss = -torch.log(output)*discounted_reward[:,None]\n",
        "    loss = Variable(loss, requires_grad=True)\n",
        "    return loss\n",
        "\n",
        "  #Search architecture for the baseline model(using dataset and dataloader)\n",
        "  def search(self,input_size,train_dataloader,REINFORCE = False):\n",
        "    for controller_epoch in range(self.controller_sampling_epochs):\n",
        "      sequences = self.sample_architrecture_sequences(self.controller_model,number_of_samples=self.samples_per_controller_epoch)\n",
        "      print(\"Evaluating architectures in controller_sampling_epoch: \",controller_epoch)\n",
        "      for i,sequence in enumerate(sequences):\n",
        "        print(\"  training architecture: \",i)\n",
        "        #train and log architecture\n",
        "        decoded_sequence = self.model_generator.decode_sequence(sequence)\n",
        "        model = self.model_generator.create_model(sequence,mlp_input_shape = input_size)\n",
        "        history = self.model_generator.train_model(model,train_dataloader)\n",
        "        self.append_model_metrics(decoded_sequence,history)\n",
        "\n",
        "      if REINFORCE == True:\n",
        "        if controller_epoch != self.controller_sampling_epochs - 1:\n",
        "          print(\"Training Controller...\")\n",
        "          #train controller\n",
        "          x,y,val_acc_target = self.prepare_controller_data(sequences)\n",
        "          self.train_control_model(self.controller_model,x,y,self.custom_loss,self.controller_training_epoch)\n",
        "      \n",
        "    return self.data\n",
        "\n",
        "  #Search architecture for the PINN model(does not use dataloader)\n",
        "  def search_PINN(self,input_size,REINFORCE = False):\n",
        "    for controller_epoch in range(self.controller_sampling_epochs):\n",
        "      sequences = self.sample_architrecture_sequences(self.controller_model,number_of_samples=self.samples_per_controller_epoch)\n",
        "      print(\"Evaluating architectures in controller_sampling_epoch: \",controller_epoch)\n",
        "      for i,sequence in enumerate(sequences):\n",
        "        print(\"  training architecture: \",i)\n",
        "        #train and log architecture\n",
        "        decoded_sequence = self.model_generator.decode_sequence(sequence)\n",
        "        model = self.model_generator.create_model(sequence,mlp_input_shape = input_size)\n",
        "        history = self.model_generator.train_PINN(model)\n",
        "\n",
        "        self.append_model_metrics(decoded_sequence,history)\n",
        "\n",
        "      if REINFORCE == True:\n",
        "        if controller_epoch != self.controller_sampling_epochs - 1:\n",
        "          print(\"Training Controller...\")\n",
        "          #train controller\n",
        "          x,y,val_acc_target = self.prepare_controller_data(sequences)\n",
        "          self.train_control_model(self.controller_model,x,y,self.custom_loss,self.controller_training_epoch)\n",
        "      \n",
        "    return self.data"
      ],
      "metadata": {
        "id": "9BK3Z1IR-NLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gF_n2cUR-To4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}